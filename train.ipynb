{"cells":[{"metadata":{"id":"920f5f1493c5b897"},"cell_type":"markdown","source":["# Ironhack Project NLP (Training)\n","\n","Team 4 (Salva, Diego, Fabi)"],"id":"920f5f1493c5b897"},{"cell_type":"markdown","source":["## Imports\n","\n","Import only needed libaries"],"metadata":{"id":"oj5MkP5199Lp"},"id":"oj5MkP5199Lp"},{"cell_type":"code","id":"7a3c3d61fc0e6e04","metadata":{"id":"7a3c3d61fc0e6e04","executionInfo":{"status":"ok","timestamp":1727969143780,"user_tz":-120,"elapsed":3451,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"ExecuteTime":{"end_time":"2024-10-02T21:49:23.017091Z","start_time":"2024-10-02T21:49:22.999896Z"}},"source":["import re\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import itertools\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB, ComplementNB\n","from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report, confusion_matrix, f1_score\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","%matplotlib inline"],"outputs":[],"execution_count":1},{"cell_type":"code","source":["nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMD5eySpWxfu","executionInfo":{"status":"ok","timestamp":1727969144284,"user_tz":-120,"elapsed":507,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"outputId":"197a0f0b-ef26-46bf-b95d-668e54023fdb","ExecuteTime":{"end_time":"2024-10-02T21:49:23.095385Z","start_time":"2024-10-02T21:49:23.081921Z"}},"id":"iMD5eySpWxfu","outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"execution_count":2},{"metadata":{"id":"2c77ddf5926c253b"},"cell_type":"markdown","source":["## Preparations"],"id":"2c77ddf5926c253b"},{"cell_type":"markdown","source":["### Constants\n","\n","Make classes readable"],"metadata":{"id":"JX7w4Zuyxceb"},"id":"JX7w4Zuyxceb"},{"cell_type":"code","source":["CLASSES = {\"0\": \"Fake news\", \"1\": \"Real news\"}"],"metadata":{"id":"xNxMAefnxfRF","executionInfo":{"status":"ok","timestamp":1727969144284,"user_tz":-120,"elapsed":19,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"ExecuteTime":{"end_time":"2024-10-02T21:49:23.203131Z","start_time":"2024-10-02T21:49:23.189781Z"}},"id":"xNxMAefnxfRF","outputs":[],"execution_count":3},{"metadata":{"id":"112732e40ae9fc7d"},"cell_type":"markdown","source":["#### Pre Processing\n","\n","Helper methods to clean up the texts"],"id":"112732e40ae9fc7d"},{"metadata":{"id":"c3e1e11203b6d78a","executionInfo":{"status":"ok","timestamp":1727969144284,"user_tz":-120,"elapsed":18,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"ExecuteTime":{"end_time":"2024-10-02T21:49:23.249926Z","start_time":"2024-10-02T21:49:23.229789Z"}},"cell_type":"code","source":["english_stopwords = stopwords.words('english')\n","def remove_stopwords(text):\n","    \"\"\"Removes stop words from a given text.\n","\n","    Args:\n","      text(string): The input text as a string.\n","\n","    Returns:\n","      The text with stop words removed as a string.\n","    \"\"\"\n","\n","    text = ' '.join([word for word in text.split() if word not in english_stopwords]) # Remove word if it's a stopword\n","    return text"],"id":"c3e1e11203b6d78a","outputs":[],"execution_count":4},{"cell_type":"code","source":["def remove_special_chars(text):\n","    \"\"\"Removes special characters from a given text.\n","\n","    Args:\n","        text(string): The input text as a string.\n","\n","    Returns:\n","        The text with special characters removed as a string.\n","    \"\"\"\n","\n","    return re.sub(r'[^\\w\\s.]', '', text) # Remove special Characters"],"metadata":{"id":"0zPszyIDbTHU","executionInfo":{"status":"ok","timestamp":1727969144285,"user_tz":-120,"elapsed":19,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}}},"id":"0zPszyIDbTHU","execution_count":5,"outputs":[]},{"metadata":{"id":"b352373604e6a2c0","executionInfo":{"status":"ok","timestamp":1727969144285,"user_tz":-120,"elapsed":19,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"ExecuteTime":{"end_time":"2024-10-02T21:49:23.327636Z","start_time":"2024-10-02T21:49:23.313292Z"}},"cell_type":"code","source":["def clean_text(text):\n","    \"\"\"\n","    Remove numbers and extra spaces from text\n","\n","    Args:\n","        text(string): The text to be cleaned.\n","\n","    Returns:\n","        string: The cleaned text.\n","    \"\"\"\n","\n","    text = re.sub(r'\\d+', '', text) # Remove numbers\n","    text = re.sub(r'\\s+', ' ', text)  # replace multiple spaces with just 1\n","    return text"],"id":"b352373604e6a2c0","outputs":[],"execution_count":6},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","def lemmatize_text(text):\n","    \"\"\"\n","    Lemmatize a given text.\n","\n","    Args:\n","        text(string): The text to be lemmatized.\n","\n","    Returns:\n","        string: The lemmatized text.\n","    \"\"\"\n","    words = word_tokenize(text) # Tokenize the text into words\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words] # Lemmatize each word\n","    return ' '.join(lemmatized_words) # Join the lemmatized words back into a single string"],"metadata":{"id":"WW1Qy1bBXMzI","executionInfo":{"status":"ok","timestamp":1727969144285,"user_tz":-120,"elapsed":18,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"ExecuteTime":{"end_time":"2024-10-02T21:49:23.343641Z","start_time":"2024-10-02T21:49:23.332281Z"}},"id":"WW1Qy1bBXMzI","outputs":[],"execution_count":7},{"metadata":{"id":"d3e7e34c02166c2d"},"cell_type":"markdown","source":["### Import Data\n","\n","Loads the labeled text data from a tab-separated file named \"training_data_lowercase.csv\" into a Pandas DataFrame."],"id":"d3e7e34c02166c2d"},{"metadata":{"collapsed":true,"id":"initial_id","executionInfo":{"status":"ok","timestamp":1727969144287,"user_tz":-120,"elapsed":18,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"ExecuteTime":{"end_time":"2024-10-02T21:49:23.545171Z","start_time":"2024-10-02T21:49:23.484978Z"}},"cell_type":"code","source":["raw_data = pd.read_csv('training_data_lowercase.csv', sep='\\t', header=None, names=['class', 'text'])"],"id":"initial_id","outputs":[],"execution_count":12},{"metadata":{"id":"ba92a85ac0d5f385"},"cell_type":"markdown","source":["### Check Data\n","\n","Check Data if it contains blank cells"],"id":"ba92a85ac0d5f385"},{"metadata":{"id":"ce2345d418325ede","outputId":"fbab01ec-2691-445b-ff21-35b9c004cf31","colab":{"base_uri":"https://localhost:8080/","height":147},"executionInfo":{"status":"ok","timestamp":1727969144287,"user_tz":-120,"elapsed":18,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"ExecuteTime":{"end_time":"2024-10-02T21:49:23.653918Z","start_time":"2024-10-02T21:49:23.632997Z"}},"cell_type":"code","source":["raw_data.isnull().sum()"],"id":"ce2345d418325ede","outputs":[{"output_type":"execute_result","data":{"text/plain":["class    0\n","text     0\n","dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>class</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>text</th>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":13}],"execution_count":13},{"metadata":{"id":"59e73c5d959a4521"},"cell_type":"markdown","source":["### Clean Data\n","\n","Create combinations of all pre prcessing methods to test the models on"],"id":"59e73c5d959a4521"},{"cell_type":"code","source":["combinations = {}\n","functions = [clean_text, remove_special_chars, remove_stopwords, lemmatize_text]\n","\n","for r in range(1, len(functions) + 1):\n","    for subset in itertools.combinations(functions, r):\n","        data_copy = raw_data.copy()\n","        func_names = [func.__name__ for func in subset]\n","        name = ','.join(func_names)\n","        print(f\"Applying: {name}\")\n","        for func in subset:\n","            data_copy['text'] = data_copy['text'].apply(func)\n","            combinations[name] = data_copy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v8ZMDZWJ9bsm","executionInfo":{"status":"ok","timestamp":1727969203162,"user_tz":-120,"elapsed":58891,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"outputId":"71847f3d-27b0-4691-9cf6-45e220b21e05"},"id":"v8ZMDZWJ9bsm","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Applying: clean_text\n","Applying: remove_special_chars\n","Applying: remove_stopwords\n","Applying: lemmatize_text\n","Applying: clean_text,remove_special_chars\n","Applying: clean_text,remove_stopwords\n","Applying: clean_text,lemmatize_text\n","Applying: remove_special_chars,remove_stopwords\n","Applying: remove_special_chars,lemmatize_text\n","Applying: remove_stopwords,lemmatize_text\n","Applying: clean_text,remove_special_chars,remove_stopwords\n","Applying: clean_text,remove_special_chars,lemmatize_text\n","Applying: clean_text,remove_stopwords,lemmatize_text\n","Applying: remove_special_chars,remove_stopwords,lemmatize_text\n","Applying: clean_text,remove_special_chars,remove_stopwords,lemmatize_text\n"]}]},{"cell_type":"markdown","source":["## Models\n","\n","Multiple models, including MultinomialNB, ComplementNB, and LogisticRegression, were tested with two vectorizers: CountVectorizer and TfidfVectorizer. These models were evaluated on different text preprocessing combinations to find the best performing combination. The goal was to identify the model and text processing approach that yielded the highest accuracy and lowest difference between training and testing accuracy."],"metadata":{"id":"CYJ91i0icDDu"},"id":"CYJ91i0icDDu"},{"cell_type":"code","source":["results = {}\n","\n","models = {\"MultinomialNB\": MultinomialNB(),\n","          \"ComplementNB\": ComplementNB(),\n","          \"LogisticRegression\": LogisticRegression(solver='liblinear')}\n","\n","vectorizers = {\"CountVectorizer\": CountVectorizer(ngram_range=(1,2)),\n","               \"TfidfVectorizer\": TfidfVectorizer(max_features=3200)}"],"metadata":{"id":"PlVQ07yrDchA"},"id":"PlVQ07yrDchA","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2024-10-02T21:49:42.645233Z","start_time":"2024-10-02T21:49:28.940082Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"e3aafb3065bb9886","executionInfo":{"status":"ok","timestamp":1727969405054,"user_tz":-120,"elapsed":201898,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"outputId":"4870deb1-eb36-454f-d4c9-19a8f9452e3d"},"cell_type":"code","source":["for name, model in models.items():\n","    for vec_name, vectorizer in vectorizers.items():\n","        for comb_name, data in combinations.items():\n","          print(name, vec_name, comb_name)\n","\n","          vec_data = vectorizer.fit_transform(data['text'])\n","          X_train, X_test, y_train, y_test = train_test_split(vec_data, data['class'], test_size=0.3, random_state=42)\n","          classifier = model\n","          classifier.fit(X_train, y_train)\n","\n","          y_pred = classifier.predict(X_test)\n","          y_pred_train = classifier.predict(X_train)\n","\n","          test_accuracy = accuracy_score(y_test, y_pred)\n","          train_accuracy = accuracy_score(y_train, y_pred_train)\n","\n","          cross_score = cross_val_score(model, vec_data, raw_data['class'], cv=5)\n","\n","          results[f\"{name}, {vec_name}, {comb_name}\"] = {\n","                \"Diff\": np.round(train_accuracy - test_accuracy, decimals=3),\n","                \"Train Accuracy\": np.round(train_accuracy, decimals=3),\n","                \"Test Accuracy\": np.round(test_accuracy, decimals=3),\n","                \"Cross Validation Score\": np.round(np.mean(cross_score), decimals=3)\n","          }\n"],"id":"e3aafb3065bb9886","outputs":[{"output_type":"stream","name":"stdout","text":["MultinomialNB CountVectorizer clean_text\n","MultinomialNB CountVectorizer remove_special_chars\n","MultinomialNB CountVectorizer remove_stopwords\n","MultinomialNB CountVectorizer lemmatize_text\n","MultinomialNB CountVectorizer clean_text,remove_special_chars\n","MultinomialNB CountVectorizer clean_text,remove_stopwords\n","MultinomialNB CountVectorizer clean_text,lemmatize_text\n","MultinomialNB CountVectorizer remove_special_chars,remove_stopwords\n","MultinomialNB CountVectorizer remove_special_chars,lemmatize_text\n","MultinomialNB CountVectorizer remove_stopwords,lemmatize_text\n","MultinomialNB CountVectorizer clean_text,remove_special_chars,remove_stopwords\n","MultinomialNB CountVectorizer clean_text,remove_special_chars,lemmatize_text\n","MultinomialNB CountVectorizer clean_text,remove_stopwords,lemmatize_text\n","MultinomialNB CountVectorizer remove_special_chars,remove_stopwords,lemmatize_text\n","MultinomialNB CountVectorizer clean_text,remove_special_chars,remove_stopwords,lemmatize_text\n","MultinomialNB TfidfVectorizer clean_text\n","MultinomialNB TfidfVectorizer remove_special_chars\n","MultinomialNB TfidfVectorizer remove_stopwords\n","MultinomialNB TfidfVectorizer lemmatize_text\n","MultinomialNB TfidfVectorizer clean_text,remove_special_chars\n","MultinomialNB TfidfVectorizer clean_text,remove_stopwords\n","MultinomialNB TfidfVectorizer clean_text,lemmatize_text\n","MultinomialNB TfidfVectorizer remove_special_chars,remove_stopwords\n","MultinomialNB TfidfVectorizer remove_special_chars,lemmatize_text\n","MultinomialNB TfidfVectorizer remove_stopwords,lemmatize_text\n","MultinomialNB TfidfVectorizer clean_text,remove_special_chars,remove_stopwords\n","MultinomialNB TfidfVectorizer clean_text,remove_special_chars,lemmatize_text\n","MultinomialNB TfidfVectorizer clean_text,remove_stopwords,lemmatize_text\n","MultinomialNB TfidfVectorizer remove_special_chars,remove_stopwords,lemmatize_text\n","MultinomialNB TfidfVectorizer clean_text,remove_special_chars,remove_stopwords,lemmatize_text\n","ComplementNB CountVectorizer clean_text\n","ComplementNB CountVectorizer remove_special_chars\n","ComplementNB CountVectorizer remove_stopwords\n","ComplementNB CountVectorizer lemmatize_text\n","ComplementNB CountVectorizer clean_text,remove_special_chars\n","ComplementNB CountVectorizer clean_text,remove_stopwords\n","ComplementNB CountVectorizer clean_text,lemmatize_text\n","ComplementNB CountVectorizer remove_special_chars,remove_stopwords\n","ComplementNB CountVectorizer remove_special_chars,lemmatize_text\n","ComplementNB CountVectorizer remove_stopwords,lemmatize_text\n","ComplementNB CountVectorizer clean_text,remove_special_chars,remove_stopwords\n","ComplementNB CountVectorizer clean_text,remove_special_chars,lemmatize_text\n","ComplementNB CountVectorizer clean_text,remove_stopwords,lemmatize_text\n","ComplementNB CountVectorizer remove_special_chars,remove_stopwords,lemmatize_text\n","ComplementNB CountVectorizer clean_text,remove_special_chars,remove_stopwords,lemmatize_text\n","ComplementNB TfidfVectorizer clean_text\n","ComplementNB TfidfVectorizer remove_special_chars\n","ComplementNB TfidfVectorizer remove_stopwords\n","ComplementNB TfidfVectorizer lemmatize_text\n","ComplementNB TfidfVectorizer clean_text,remove_special_chars\n","ComplementNB TfidfVectorizer clean_text,remove_stopwords\n","ComplementNB TfidfVectorizer clean_text,lemmatize_text\n","ComplementNB TfidfVectorizer remove_special_chars,remove_stopwords\n","ComplementNB TfidfVectorizer remove_special_chars,lemmatize_text\n","ComplementNB TfidfVectorizer remove_stopwords,lemmatize_text\n","ComplementNB TfidfVectorizer clean_text,remove_special_chars,remove_stopwords\n","ComplementNB TfidfVectorizer clean_text,remove_special_chars,lemmatize_text\n","ComplementNB TfidfVectorizer clean_text,remove_stopwords,lemmatize_text\n","ComplementNB TfidfVectorizer remove_special_chars,remove_stopwords,lemmatize_text\n","ComplementNB TfidfVectorizer clean_text,remove_special_chars,remove_stopwords,lemmatize_text\n","LogisticRegression CountVectorizer clean_text\n","LogisticRegression CountVectorizer remove_special_chars\n","LogisticRegression CountVectorizer remove_stopwords\n","LogisticRegression CountVectorizer lemmatize_text\n","LogisticRegression CountVectorizer clean_text,remove_special_chars\n","LogisticRegression CountVectorizer clean_text,remove_stopwords\n","LogisticRegression CountVectorizer clean_text,lemmatize_text\n","LogisticRegression CountVectorizer remove_special_chars,remove_stopwords\n","LogisticRegression CountVectorizer remove_special_chars,lemmatize_text\n","LogisticRegression CountVectorizer remove_stopwords,lemmatize_text\n","LogisticRegression CountVectorizer clean_text,remove_special_chars,remove_stopwords\n","LogisticRegression CountVectorizer clean_text,remove_special_chars,lemmatize_text\n","LogisticRegression CountVectorizer clean_text,remove_stopwords,lemmatize_text\n","LogisticRegression CountVectorizer remove_special_chars,remove_stopwords,lemmatize_text\n","LogisticRegression CountVectorizer clean_text,remove_special_chars,remove_stopwords,lemmatize_text\n","LogisticRegression TfidfVectorizer clean_text\n","LogisticRegression TfidfVectorizer remove_special_chars\n","LogisticRegression TfidfVectorizer remove_stopwords\n","LogisticRegression TfidfVectorizer lemmatize_text\n","LogisticRegression TfidfVectorizer clean_text,remove_special_chars\n","LogisticRegression TfidfVectorizer clean_text,remove_stopwords\n","LogisticRegression TfidfVectorizer clean_text,lemmatize_text\n","LogisticRegression TfidfVectorizer remove_special_chars,remove_stopwords\n","LogisticRegression TfidfVectorizer remove_special_chars,lemmatize_text\n","LogisticRegression TfidfVectorizer remove_stopwords,lemmatize_text\n","LogisticRegression TfidfVectorizer clean_text,remove_special_chars,remove_stopwords\n","LogisticRegression TfidfVectorizer clean_text,remove_special_chars,lemmatize_text\n","LogisticRegression TfidfVectorizer clean_text,remove_stopwords,lemmatize_text\n","LogisticRegression TfidfVectorizer remove_special_chars,remove_stopwords,lemmatize_text\n","LogisticRegression TfidfVectorizer clean_text,remove_special_chars,remove_stopwords,lemmatize_text\n"]}],"execution_count":15},{"cell_type":"code","source":["results_df = pd.DataFrame(results).T\n","results_df.sort_values(by=['Train Accuracy', 'Diff'], ascending=[False, True])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"rZ3Qywl2V-A7","executionInfo":{"status":"ok","timestamp":1727969405055,"user_tz":-120,"elapsed":13,"user":{"displayName":"Fabian Hieber","userId":"06681858431281640040"}},"outputId":"49932fd7-1759-41e6-bdb6-35be2710e55d"},"id":"rZ3Qywl2V-A7","execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                     Diff  Train Accuracy  \\\n","LogisticRegression, CountVectorizer, clean_text     0.049           0.999   \n","LogisticRegression, CountVectorizer, lemmatize_...  0.049           0.999   \n","LogisticRegression, CountVectorizer, clean_text...  0.049           0.999   \n","LogisticRegression, CountVectorizer, remove_spe...  0.049           0.999   \n","LogisticRegression, CountVectorizer, remove_spe...  0.050           0.999   \n","...                                                   ...             ...   \n","MultinomialNB, TfidfVectorizer, remove_special_...  0.013           0.931   \n","MultinomialNB, TfidfVectorizer, clean_text,remo...  0.013           0.931   \n","MultinomialNB, TfidfVectorizer, clean_text,remo...  0.008           0.930   \n","ComplementNB, TfidfVectorizer, remove_special_c...  0.008           0.930   \n","MultinomialNB, TfidfVectorizer, remove_special_...  0.009           0.930   \n","\n","                                                    Test Accuracy  \\\n","LogisticRegression, CountVectorizer, clean_text             0.949   \n","LogisticRegression, CountVectorizer, lemmatize_...          0.949   \n","LogisticRegression, CountVectorizer, clean_text...          0.949   \n","LogisticRegression, CountVectorizer, remove_spe...          0.949   \n","LogisticRegression, CountVectorizer, remove_spe...          0.949   \n","...                                                           ...   \n","MultinomialNB, TfidfVectorizer, remove_special_...          0.918   \n","MultinomialNB, TfidfVectorizer, clean_text,remo...          0.919   \n","MultinomialNB, TfidfVectorizer, clean_text,remo...          0.922   \n","ComplementNB, TfidfVectorizer, remove_special_c...          0.922   \n","MultinomialNB, TfidfVectorizer, remove_special_...          0.922   \n","\n","                                                    Cross Validation Score  \n","LogisticRegression, CountVectorizer, clean_text                      0.922  \n","LogisticRegression, CountVectorizer, lemmatize_...                   0.922  \n","LogisticRegression, CountVectorizer, clean_text...                   0.923  \n","LogisticRegression, CountVectorizer, remove_spe...                   0.920  \n","LogisticRegression, CountVectorizer, remove_spe...                   0.920  \n","...                                                                    ...  \n","MultinomialNB, TfidfVectorizer, remove_special_...                   0.895  \n","MultinomialNB, TfidfVectorizer, clean_text,remo...                   0.896  \n","MultinomialNB, TfidfVectorizer, clean_text,remo...                   0.895  \n","ComplementNB, TfidfVectorizer, remove_special_c...                   0.896  \n","MultinomialNB, TfidfVectorizer, remove_special_...                   0.894  \n","\n","[90 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-5568a850-d07b-4248-9739-c94d8feb9d62\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Diff</th>\n","      <th>Train Accuracy</th>\n","      <th>Test Accuracy</th>\n","      <th>Cross Validation Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>LogisticRegression, CountVectorizer, clean_text</th>\n","      <td>0.049</td>\n","      <td>0.999</td>\n","      <td>0.949</td>\n","      <td>0.922</td>\n","    </tr>\n","    <tr>\n","      <th>LogisticRegression, CountVectorizer, lemmatize_text</th>\n","      <td>0.049</td>\n","      <td>0.999</td>\n","      <td>0.949</td>\n","      <td>0.922</td>\n","    </tr>\n","    <tr>\n","      <th>LogisticRegression, CountVectorizer, clean_text,lemmatize_text</th>\n","      <td>0.049</td>\n","      <td>0.999</td>\n","      <td>0.949</td>\n","      <td>0.923</td>\n","    </tr>\n","    <tr>\n","      <th>LogisticRegression, CountVectorizer, remove_special_chars,lemmatize_text</th>\n","      <td>0.049</td>\n","      <td>0.999</td>\n","      <td>0.949</td>\n","      <td>0.920</td>\n","    </tr>\n","    <tr>\n","      <th>LogisticRegression, CountVectorizer, remove_special_chars</th>\n","      <td>0.050</td>\n","      <td>0.999</td>\n","      <td>0.949</td>\n","      <td>0.920</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>MultinomialNB, TfidfVectorizer, remove_special_chars,remove_stopwords,lemmatize_text</th>\n","      <td>0.013</td>\n","      <td>0.931</td>\n","      <td>0.918</td>\n","      <td>0.895</td>\n","    </tr>\n","    <tr>\n","      <th>MultinomialNB, TfidfVectorizer, clean_text,remove_special_chars,remove_stopwords,lemmatize_text</th>\n","      <td>0.013</td>\n","      <td>0.931</td>\n","      <td>0.919</td>\n","      <td>0.896</td>\n","    </tr>\n","    <tr>\n","      <th>MultinomialNB, TfidfVectorizer, clean_text,remove_special_chars,remove_stopwords</th>\n","      <td>0.008</td>\n","      <td>0.930</td>\n","      <td>0.922</td>\n","      <td>0.895</td>\n","    </tr>\n","    <tr>\n","      <th>ComplementNB, TfidfVectorizer, remove_special_chars,remove_stopwords</th>\n","      <td>0.008</td>\n","      <td>0.930</td>\n","      <td>0.922</td>\n","      <td>0.896</td>\n","    </tr>\n","    <tr>\n","      <th>MultinomialNB, TfidfVectorizer, remove_special_chars,remove_stopwords</th>\n","      <td>0.009</td>\n","      <td>0.930</td>\n","      <td>0.922</td>\n","      <td>0.894</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>90 rows × 4 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5568a850-d07b-4248-9739-c94d8feb9d62')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5568a850-d07b-4248-9739-c94d8feb9d62 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5568a850-d07b-4248-9739-c94d8feb9d62');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-48408a73-7f57-4a91-8d48-80594901076a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-48408a73-7f57-4a91-8d48-80594901076a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-48408a73-7f57-4a91-8d48-80594901076a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"results_df\",\n  \"rows\": 90,\n  \"fields\": [\n    {\n      \"column\": \"Diff\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018522412782380673,\n        \"min\": 0.004,\n        \"max\": 0.056,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          0.038,\n          0.045,\n          0.039\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027047396786176358,\n        \"min\": 0.93,\n        \"max\": 0.999,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.999,\n          0.933,\n          0.935\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009604026673201961,\n        \"min\": 0.918,\n        \"max\": 0.949,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          0.928,\n          0.938,\n          0.949\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cross Validation Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009218826744321798,\n        \"min\": 0.894,\n        \"max\": 0.925,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          0.899,\n          0.913,\n          0.922\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":16}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[],"machine_shape":"hm"}},"nbformat":4,"nbformat_minor":5}